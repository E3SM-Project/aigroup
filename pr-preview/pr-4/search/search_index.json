{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to AIG","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This space will serve as a collective working space for E3SM AIG. We will share docs, scripts, examples, prototypes for e3sm AI efforts.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#workflows","title":"Workflows","text":"<ul> <li>ACE2-ERA5 Training Workflow - Complete guide for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset on GPU compute nodes.</li> </ul> <p>Did you know?</p> <p>We also share our meetings notes on confluence?!</p>"},{"location":"ace2-workflow/","title":"ACE2-ERA5 Training Workflow","text":"<p>This guide provides a complete workflow for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset.</p>"},{"location":"ace2-workflow/#overview","title":"Overview","text":"<p>ACE is a machine learning model for climate emulation developed by AI2. This workflow will guide you through setting up the environment and running training on a GPU compute node.</p>"},{"location":"ace2-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a compute cluster with GPU nodes (e.g., pm-gpu)</li> <li>Storage space for the dataset (ACE2-ERA5)</li> <li>Network access to clone repositories</li> </ul>"},{"location":"ace2-workflow/#resources","title":"Resources","text":"<ul> <li>Code Repository: E3SM-Project/ace</li> <li>Dataset: ACE2-ERA5 on Hugging Face</li> <li>Documentation: ACE Training Configuration Guide</li> </ul>"},{"location":"ace2-workflow/#setup-instructions","title":"Setup Instructions","text":""},{"location":"ace2-workflow/#1-clone-the-code-repository","title":"1. Clone the Code Repository","text":"<p>Start by cloning the ACE repository and checking out the main branch:</p> <pre><code>git clone https://github.com/E3SM-Project/ace.git\ncd ace\n</code></pre>"},{"location":"ace2-workflow/#2-download-the-dataset","title":"2. Download the Dataset","text":"<p>Clone the ACE2-ERA5 dataset from Hugging Face:</p> <pre><code>git clone https://huggingface.co/allenai/ACE2-ERA5\n</code></pre> <p>Dataset Size</p> <p>The ACE2-ERA5 dataset is large. Ensure you have sufficient storage space before cloning.</p>"},{"location":"ace2-workflow/#3-install-uv-package-manager","title":"3. Install uv Package Manager","text":"<p>Install the <code>uv</code> package manager, which is used to manage Python dependencies:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, you may need to restart your shell or source your profile to use <code>uv</code>.</p>"},{"location":"ace2-workflow/#4-configure-uv-cache","title":"4. Configure uv Cache","text":"<p>Set up the uv cache directory in an accessible location with sufficient storage:</p> <pre><code>mkdir -p \"$PSCRATCH/.cache/uv\"\nexport UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"\n</code></pre> <p>Cache Location</p> <p>Adjust <code>$PSCRATCH</code> to match your system's scratch directory path. This ensures the cache is stored in a location with adequate space.</p>"},{"location":"ace2-workflow/#5-pin-python-version","title":"5. Pin Python Version","text":"<p>Pin the Python version to 3.11:</p> <pre><code>uv python pin 3.11\n</code></pre>"},{"location":"ace2-workflow/#running-training","title":"Running Training","text":""},{"location":"ace2-workflow/#1-request-a-gpu-compute-node","title":"1. Request a GPU Compute Node","text":"<p>Request an interactive GPU node on your cluster:</p> <pre><code>salloc --nodes 1 --qos interactive --time 04:00:00 --constraint gpu --account=e3sm_g\n</code></pre> <p>Account Settings</p> <p>Adjust the <code>--account</code> parameter to match your allocation account.</p>"},{"location":"ace2-workflow/#2-prepare-training-configuration","title":"2. Prepare Training Configuration","text":"<p>Create a training configuration file named <code>config-train.yaml</code> in the repository root. You can start with a template from the ACE training configuration documentation.</p> <p>Important: Make sure to update the following in your <code>config-train.yaml</code>: - <code>experiment_dir</code>: Set this to a writable directory where training outputs will be saved - <code>data_path</code>: Point this to your downloaded ACE2-ERA5 dataset location</p> <p>Fast Iteration</p> <p>For faster iteration during initial testing, consider: - Reducing the number of training epochs - Using a smaller batch size - Limiting the dataset size</p>"},{"location":"ace2-workflow/#3-launch-training","title":"3. Launch Training","text":"<p>From the repository root, launch the training job using <code>torchrun</code>:</p> <pre><code>uv run torchrun --nproc_per_node=4 -m fme.ace.train config-train.yaml\n</code></pre> <p>This command will: - Use <code>uv run</code> to manage dependencies automatically - Launch <code>torchrun</code> with 4 processes (one per GPU) - Execute the training module with your configuration</p>"},{"location":"ace2-workflow/#training-parameters","title":"Training Parameters","text":"<p>The <code>torchrun</code> command accepts several parameters:</p> <ul> <li><code>--nproc_per_node=4</code>: Number of processes per node (typically matches the number of GPUs)</li> <li><code>-m fme.ace.train</code>: The Python module to run</li> <li><code>config-train.yaml</code>: Your training configuration file</li> </ul>"},{"location":"ace2-workflow/#monitoring-training","title":"Monitoring Training","text":"<p>During training, monitor: - GPU utilization: Use <code>nvidia-smi</code> to check GPU usage - Training logs: Check the output directory specified in <code>experiment_dir</code> - Checkpoints: Models will be saved periodically based on your configuration</p>"},{"location":"ace2-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ace2-workflow/#common-issues","title":"Common Issues","text":"<p>Out of Memory Errors - Reduce batch size in <code>config-train.yaml</code> - Decrease model size parameters - Use fewer GPUs with <code>--nproc_per_node</code></p> <p>Cache Directory Issues - Ensure <code>UV_CACHE_DIR</code> has sufficient space - Check write permissions on the cache directory</p> <p>Module Import Errors - Verify Python version is pinned to 3.11 - Ensure you're running from the repository root - Check that dependencies are properly installed by <code>uv</code></p>"},{"location":"ace2-workflow/#next-steps","title":"Next Steps","text":"<p>After successful training: - Evaluate model performance using validation scripts - Export trained models for inference - Visualize training metrics and results - Fine-tune hyperparameters based on initial results</p>"},{"location":"ace2-workflow/#additional-resources","title":"Additional Resources","text":"<ul> <li>ACE Documentation</li> <li>uv Documentation</li> <li>PyTorch Distributed Training</li> </ul>"}]}