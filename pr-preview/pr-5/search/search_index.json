{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#e3sm-ai-group","title":"E3SM AI Group","text":"Get Started GitHub"},{"location":"#introduction","title":"Introduction","text":"<p>This space serves as a collective working space for E3SM AIG. We share docs, scripts, examples, and prototypes for E3SM AI efforts.</p> <p>Did you know?</p> <p>We also share our meeting notes internally on Confluence!</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#workflows","title":"Workflows","text":"<ul> <li>ACE2-ERA5 Training Workflow - Complete guide for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset on GPU compute nodes.</li> </ul>"},{"location":"ace2-workflow/","title":"ACE2-ERA5 Training Workflow","text":"<p>This guide provides a complete workflow for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset.</p>"},{"location":"ace2-workflow/#overview","title":"Overview","text":"<p>ACE is a machine learning model for climate emulation developed by AI2. This workflow will guide you through setting up the environment and running training on a GPU compute node.</p>"},{"location":"ace2-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a compute cluster with GPU nodes (e.g., pm-gpu)</li> <li>Storage space for the dataset (ACE2-ERA5)</li> <li>Network access to clone repositories</li> </ul>"},{"location":"ace2-workflow/#resources","title":"Resources","text":"<ul> <li>Code Repository: E3SM-Project/ace</li> <li>Dataset: ACE2-ERA5 on Hugging Face</li> <li>Documentation: ACE Training Configuration Guide</li> </ul>"},{"location":"ace2-workflow/#setup-instructions","title":"Setup Instructions","text":""},{"location":"ace2-workflow/#1-clone-the-code-repository","title":"1. Clone the Code Repository","text":"<p>Start by cloning the ACE repository and checking out the main branch:</p> <pre><code>git clone https://github.com/E3SM-Project/ace.git\ncd ace\n</code></pre>"},{"location":"ace2-workflow/#2-download-the-dataset","title":"2. Download the Dataset","text":"<p>Clone the ACE2-ERA5 dataset from Hugging Face:</p> <pre><code>git clone https://huggingface.co/allenai/ACE2-ERA5\n</code></pre> <p>Dataset Size</p> <p>The ACE2-ERA5 dataset is large. Ensure you have sufficient storage space before cloning. Also, if you run into issues related to git lfs, you may need to install that.</p>"},{"location":"ace2-workflow/#3-install-uv-package-manager","title":"3. Install uv Package Manager","text":"<p>Install the <code>uv</code> package manager, which is used to manage Python dependencies:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, you may need to restart your console or source your profile to use <code>uv</code>.</p>"},{"location":"ace2-workflow/#4-configure-uv-cache","title":"4. Configure uv Cache","text":"<p>Set up the uv cache directory in an accessible location with sufficient storage:</p> <pre><code>mkdir -p \"$PSCRATCH/.cache/uv\"\nexport UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"\n</code></pre> <p>Cache Location</p> <p>Adjust <code>$PSCRATCH</code> to match your system's scratch directory path. This ensures the cache is stored in a location with adequate space.</p>"},{"location":"ace2-workflow/#5-pin-python-version","title":"5. Pin Python Version","text":"<p>Pin the Python version to 3.11:</p> <pre><code>uv python pin 3.11\n</code></pre>"},{"location":"ace2-workflow/#running-training","title":"Running Training","text":""},{"location":"ace2-workflow/#1-request-a-gpu-compute-node","title":"1. Request a GPU Compute Node","text":"<p>Request an interactive GPU node on your cluster:</p> <pre><code>salloc --nodes 1 --qos interactive --time 04:00:00 --constraint gpu --account=e3sm_g\n</code></pre> <p>Account Settings</p> <p>Adjust the <code>--account</code> parameter to match your allocation account.</p>"},{"location":"ace2-workflow/#2-prepare-training-configuration","title":"2. Prepare Training Configuration","text":"<p>Create a training configuration file named <code>config-train.yaml</code> in the repository root. You can start with a template from the ACE training configuration documentation, or use the sample below:</p> Sample Configuration (<code>config-train.yaml</code>) <pre><code>experiment_dir: /path/to/your/ACE2-ERA5/train_output\nsave_checkpoint: true\nvalidate_using_ema: true\nmax_epochs: 80\nn_forward_steps: 1\ninference:\n  n_forward_steps: 300  # ~75 days (adjust based on your needs)\n  forward_steps_in_memory: 1\n  loader:\n    start_indices:\n      first: 0\n      n_initial_conditions: 4\n      interval: 300  # adjusted to fit within dataset\n    dataset:\n      data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\n    num_data_workers: 4\nlogging:\n  log_to_screen: true\n  log_to_wandb: false\n  log_to_file: true\n  project: ace\n  entity: your_wandb_entity\ntrain_loader:\n  batch_size: 4\n  num_data_workers: 2\n  prefetch_factor: 2\n  dataset:\n    concat:\n      - data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\nvalidation_loader:\n  batch_size: 4\n  num_data_workers: 2\n  prefetch_factor: 2\n  dataset:\n    data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\n    subset:\n      step: 5\noptimization:\n  enable_automatic_mixed_precision: false\n  lr: 0.0001\n  optimizer_type: AdamW\n  # can also set kwargs: fused: true for performance if using GPU\nstepper:\n  loss:\n    type: MSE\n  step:\n    type: single_module\n    config:\n      builder:\n        type: SphericalFourierNeuralOperatorNet\n        config:\n          embed_dim: 16\n          filter_type: linear\n          hard_thresholding_fraction: 1.0\n          use_mlp: true\n          normalization_layer: instance_norm\n          num_layers: 2\n          operator_type: dhconv\n          scale_factor: 1\n          separable: false\n      normalization:\n        network:\n          global_means_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/centering.nc\n          global_stds_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/scaling-full-field.nc\n        loss:\n          global_means_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/centering.nc\n          global_stds_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/scaling-residual.nc\n      in_names:\n      - land_fraction\n      - ocean_fraction\n      - sea_ice_fraction\n      - DSWRFtoa\n      - HGTsfc\n      - PRESsfc\n      - surface_temperature\n      - air_temperature_0 # _0 denotes the top most layer of the atmosphere\n      - air_temperature_1\n      - air_temperature_2\n      - air_temperature_3\n      - air_temperature_4\n      - air_temperature_5\n      - air_temperature_6\n      - air_temperature_7\n      - specific_total_water_0\n      - specific_total_water_1\n      - specific_total_water_2\n      - specific_total_water_3\n      - specific_total_water_4\n      - specific_total_water_5\n      - specific_total_water_6\n      - specific_total_water_7\n      - eastward_wind_0\n      - eastward_wind_1\n      - eastward_wind_2\n      - eastward_wind_3\n      - eastward_wind_4\n      - eastward_wind_5\n      - eastward_wind_6\n      - eastward_wind_7\n      - northward_wind_0\n      - northward_wind_1\n      - northward_wind_2\n      - northward_wind_3\n      - northward_wind_4\n      - northward_wind_5\n      - northward_wind_6\n      - northward_wind_7\n      out_names:\n      - PRESsfc\n      - surface_temperature\n      - air_temperature_0\n      - air_temperature_1\n      - air_temperature_2\n      - air_temperature_3\n      - air_temperature_4\n      - air_temperature_5\n      - air_temperature_6\n      - air_temperature_7\n      - specific_total_water_0\n      - specific_total_water_1\n      - specific_total_water_2\n      - specific_total_water_3\n      - specific_total_water_4\n      - specific_total_water_5\n      - specific_total_water_6\n      - specific_total_water_7\n      - eastward_wind_0\n      - eastward_wind_1\n      - eastward_wind_2\n      - eastward_wind_3\n      - eastward_wind_4\n      - eastward_wind_5\n      - eastward_wind_6\n      - eastward_wind_7\n      - northward_wind_0\n      - northward_wind_1\n      - northward_wind_2\n      - northward_wind_3\n      - northward_wind_4\n      - northward_wind_5\n      - northward_wind_6\n      - northward_wind_7\n      - LHTFLsfc\n      - SHTFLsfc\n      - PRATEsfc\n      - ULWRFsfc\n      - ULWRFtoa\n      - DLWRFsfc\n      - DSWRFsfc\n      - USWRFsfc\n      - USWRFtoa\n      - tendency_of_total_water_path_due_to_advection\n</code></pre> <p>Important: Make sure to update the following in your <code>config-train.yaml</code>:</p> <ul> <li><code>experiment_dir</code>: Set this to a writable directory where training outputs will be saved</li> <li><code>data_path</code>: Point this to your downloaded ACE2-ERA5 dataset location</li> </ul> <p>Fast Iteration</p> <p>For faster iteration during initial testing, consider:</p> <ul> <li>Reducing the number of training epochs</li> <li>Using a smaller batch size</li> <li>Limiting the dataset size</li> </ul>"},{"location":"ace2-workflow/#3-launch-training","title":"3. Launch Training","text":"<p>From the repository root, launch the training job using <code>torchrun</code>:</p> <pre><code>uv run torchrun --nproc_per_node=4 -m fme.ace.train config-train.yaml\n</code></pre> <p>This command will:</p> <ul> <li>Use <code>uv run</code> to manage dependencies automatically</li> <li>Launch <code>torchrun</code> with 4 processes (one per GPU)</li> <li>Execute the training module with your configuration</li> </ul>"},{"location":"ace2-workflow/#training-parameters","title":"Training Parameters","text":"<p>The <code>torchrun</code> command accepts several parameters:</p> <ul> <li><code>--nproc_per_node=4</code>: Number of processes per node (typically matches the number of GPUs)</li> <li><code>-m fme.ace.train</code>: The Python module to run</li> <li><code>config-train.yaml</code>: Your training configuration file</li> </ul>"},{"location":"ace2-workflow/#monitoring-training","title":"Monitoring Training","text":"<p>During training, monitor:</p> <ul> <li>GPU utilization: Use <code>nvidia-smi</code> to check GPU usage</li> <li>Training logs: Check the output directory specified in <code>experiment_dir</code></li> <li>Checkpoints: Models will be saved periodically based on your configuration</li> </ul>"},{"location":"ace2-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ace2-workflow/#common-issues","title":"Common Issues","text":"<p>Out of Memory Errors</p> <ul> <li>Reduce batch size in <code>config-train.yaml</code></li> <li>Decrease model size parameters</li> <li>Use fewer GPUs with <code>--nproc_per_node</code></li> </ul> <p>Cache Directory Issues</p> <ul> <li>Ensure <code>UV_CACHE_DIR</code> has sufficient space</li> <li>Check write permissions on the cache directory</li> </ul> <p>Module Import Errors</p> <ul> <li>Verify Python version is pinned to 3.11</li> <li>Ensure you're running from the repository root</li> <li>Check that dependencies are properly installed by <code>uv</code></li> </ul>"},{"location":"ace2-workflow/#next-steps","title":"Next Steps","text":"<p>After successful training:</p> <ul> <li>Evaluate model performance using validation scripts</li> <li>Export trained models for inference</li> <li>Visualize training metrics and results</li> <li>Fine-tune hyperparameters based on initial results</li> </ul>"},{"location":"ace2-workflow/#additional-resources","title":"Additional Resources","text":"<ul> <li>ACE Documentation</li> <li>uv Documentation</li> <li>PyTorch Distributed Training</li> </ul>"}]}