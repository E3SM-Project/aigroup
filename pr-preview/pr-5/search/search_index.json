{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#e3sm-ai-group","title":"E3SM AI Group","text":"Get Started GitHub"},{"location":"#introduction","title":"Introduction","text":"<p>This space serves as a collective working space for E3SM AIG. We share docs, scripts, examples, and prototypes for E3SM AI efforts.</p> <p>Did you know?</p> <p>We also share our meeting notes internally on Confluence!</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#workflows","title":"Workflows","text":"<ul> <li>ACE2-ERA5 Training Workflow - Complete guide for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset on GPU compute nodes.</li> </ul>"},{"location":"ace2-workflow/","title":"ACE2-ERA5 Training Workflow","text":"<p>This guide provides a complete workflow for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset.</p>"},{"location":"ace2-workflow/#overview","title":"Overview","text":"<p>ACE is a machine learning model for climate emulation developed by AI2. This workflow will guide you through setting up the environment and running training on a GPU compute node.</p>"},{"location":"ace2-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a compute cluster with GPU nodes (e.g., pm-gpu)</li> <li>Storage space for the dataset (ACE2-ERA5)</li> <li>Network access to clone repositories</li> </ul>"},{"location":"ace2-workflow/#resources","title":"Resources","text":"<ul> <li>Code Repository: E3SM-Project/ace</li> <li>Dataset: ACE2-ERA5 on Hugging Face</li> <li>Documentation: ACE Training Configuration Guide</li> </ul>"},{"location":"ace2-workflow/#setup-instructions","title":"Setup Instructions","text":""},{"location":"ace2-workflow/#1-clone-the-code-repository","title":"1. Clone the Code Repository","text":"<p>Start by cloning the ACE repository and checking out the main branch:</p> <pre><code>git clone https://github.com/E3SM-Project/ace.git\ncd ace\n</code></pre>"},{"location":"ace2-workflow/#2-download-the-dataset","title":"2. Download the Dataset","text":"<p>Clone the ACE2-ERA5 dataset from Hugging Face:</p> <pre><code>git clone https://huggingface.co/allenai/ACE2-ERA5\n</code></pre> <p>Dataset Size</p> <p>The ACE2-ERA5 dataset is large. Ensure you have sufficient storage space before cloning. Also, if you run into issues related to git lfs, you may need to install that.</p>"},{"location":"ace2-workflow/#3-install-uv-package-manager","title":"3. Install uv Package Manager","text":"<p>Install the <code>uv</code> package manager, which is used to manage Python dependencies:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, you may need to restart your console or source your profile to use <code>uv</code>.</p>"},{"location":"ace2-workflow/#4-configure-uv-cache","title":"4. Configure uv Cache","text":"<p>Set up the uv cache directory in an accessible location with sufficient storage:</p> <pre><code>mkdir -p \"$PSCRATCH/.cache/uv\"\nexport UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"\n</code></pre> <p>Cache Location</p> <p>Adjust <code>$PSCRATCH</code> to match your system's scratch directory path. This ensures the cache is stored in a location with adequate space.</p>"},{"location":"ace2-workflow/#5-pin-python-version","title":"5. Pin Python Version","text":"<p>Pin the Python version to 3.11:</p> <pre><code>uv python pin 3.11\n</code></pre>"},{"location":"ace2-workflow/#running-training","title":"Running Training","text":""},{"location":"ace2-workflow/#1-request-a-gpu-compute-node","title":"1. Request a GPU Compute Node","text":"<p>Request an interactive GPU node on your cluster:</p> <pre><code>salloc --nodes 1 --qos interactive --time 04:00:00 --constraint gpu --account=e3sm_g\n</code></pre> <p>Account Settings</p> <p>Adjust the <code>--account</code> parameter to match your allocation account.</p>"},{"location":"ace2-workflow/#2-prepare-training-configuration","title":"2. Prepare Training Configuration","text":"<p>Create a training configuration file named <code>config-train.yaml</code> in the repository root. You can start with a template from the ACE training configuration documentation.</p> <p>Important: Make sure to update the following in your <code>config-train.yaml</code>:</p> <ul> <li><code>experiment_dir</code>: Set this to a writable directory where training outputs will be saved</li> <li><code>data_path</code>: Point this to your downloaded ACE2-ERA5 dataset location</li> </ul> <p>Fast Iteration</p> <p>For faster iteration during initial testing, consider:</p> <ul> <li>Reducing the number of training epochs</li> <li>Using a smaller batch size</li> <li>Limiting the dataset size</li> </ul>"},{"location":"ace2-workflow/#3-launch-training","title":"3. Launch Training","text":"<p>From the repository root, launch the training job using <code>torchrun</code>:</p> <pre><code>uv run torchrun --nproc_per_node=4 -m fme.ace.train config-train.yaml\n</code></pre> <p>This command will:</p> <ul> <li>Use <code>uv run</code> to manage dependencies automatically</li> <li>Launch <code>torchrun</code> with 4 processes (one per GPU)</li> <li>Execute the training module with your configuration</li> </ul>"},{"location":"ace2-workflow/#training-parameters","title":"Training Parameters","text":"<p>The <code>torchrun</code> command accepts several parameters:</p> <ul> <li><code>--nproc_per_node=4</code>: Number of processes per node (typically matches the number of GPUs)</li> <li><code>-m fme.ace.train</code>: The Python module to run</li> <li><code>config-train.yaml</code>: Your training configuration file</li> </ul>"},{"location":"ace2-workflow/#monitoring-training","title":"Monitoring Training","text":"<p>During training, monitor:</p> <ul> <li>GPU utilization: Use <code>nvidia-smi</code> to check GPU usage</li> <li>Training logs: Check the output directory specified in <code>experiment_dir</code></li> <li>Checkpoints: Models will be saved periodically based on your configuration</li> </ul>"},{"location":"ace2-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ace2-workflow/#common-issues","title":"Common Issues","text":"<p>Out of Memory Errors</p> <ul> <li>Reduce batch size in <code>config-train.yaml</code></li> <li>Decrease model size parameters</li> <li>Use fewer GPUs with <code>--nproc_per_node</code></li> </ul> <p>Cache Directory Issues</p> <ul> <li>Ensure <code>UV_CACHE_DIR</code> has sufficient space</li> <li>Check write permissions on the cache directory</li> </ul> <p>Module Import Errors</p> <ul> <li>Verify Python version is pinned to 3.11</li> <li>Ensure you're running from the repository root</li> <li>Check that dependencies are properly installed by <code>uv</code></li> </ul>"},{"location":"ace2-workflow/#next-steps","title":"Next Steps","text":"<p>After successful training:</p> <ul> <li>Evaluate model performance using validation scripts</li> <li>Export trained models for inference</li> <li>Visualize training metrics and results</li> <li>Fine-tune hyperparameters based on initial results</li> </ul>"},{"location":"ace2-workflow/#additional-resources","title":"Additional Resources","text":"<ul> <li>ACE Documentation</li> <li>uv Documentation</li> <li>PyTorch Distributed Training</li> </ul>"}]}