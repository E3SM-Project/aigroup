{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"E3SM AI Group","text":""},{"location":"#ai-group","title":"AI Group","text":"Get Started"},{"location":"#introduction","title":"Introduction","text":"<p>This space serves as a collective working space for E3SM AI Group. We share docs, scripts, examples, and prototypes for E3SM AI efforts.</p> <p>Did you know?</p> <p>We also share our meeting notes internally on Confluence!</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To access our quick guides ane examples, click on the Quick Guides tab at the top of the page.</p>"},{"location":"ace2-inference/","title":"ACE2 Inference Tutorial","text":"<p>This guide walks you through running inference using the pre-trained ACE2-EAMv3 model.</p>"},{"location":"ace2-inference/#prerequisites-for-this-guide","title":"Prerequisites for this guide","text":"<ul> <li>uv installed to set up the environment, including Py Torch. See our Python Environment Setup for more details.</li> <li>Access to the internet to clone repositories</li> </ul>"},{"location":"ace2-inference/#steps","title":"Steps","text":""},{"location":"ace2-inference/#1-clone-the-ace-repository","title":"1. Clone the ACE Repository","text":"<p>Clone the ACE repository from GitHub:</p> <pre><code>git clone https://github.com/E3SM-Project/ace\n</code></pre>"},{"location":"ace2-inference/#2-clone-the-model-repository","title":"2. Clone the Model Repository","text":"<p>Clone the ACE2-EAMv3 model repository from Hugging Face:</p> <pre><code>git clone https://huggingface.co/allenai/ACE2-EAMv3\n</code></pre> <p>git lfs</p> <p>If you run into issues related to git lfs, you may need to install that.</p>"},{"location":"ace2-inference/#3-run-inference","title":"3. Run Inference","text":"<p>Navigate to the <code>ace</code> repository directory:</p> <pre><code>cd ace\n</code></pre> <p>Create a configuration file named <code>config-inference.yaml</code> with the following content. Make sure to update the paths to match your environment.</p> <pre><code>experiment_dir: /pscratch/sd/m/mahf708/ACE2-EAMv3/test1 # (1)!\nn_forward_steps: 1458 # (2)!\nforward_steps_in_memory: 80 # (3)!\ncheckpoint_path: /pscratch/sd/m/mahf708/ACE2-EAMv3/ace2_EAMv3_ckpt.tar # (4)!\ninitial_condition: # (5)!\n  path: /pscratch/sd/m/mahf708/ACE2-EAMv3/initial_conditions/1971010100.nc\n  start_indices:\n    n_initial_conditions: 2\n    first: 0\n    interval: 1\nforcing_loader: # (6)!\n  dataset:\n    data_path: /pscratch/sd/m/mahf708/ACE2-EAMv3/forcing_data/\n  num_data_workers: 2\nlogging: # (7)!\n  log_to_screen: true\n  log_to_wandb: false\n  log_to_file: true\ndata_writer: # (8)!\n  save_prediction_files: true\n</code></pre> <ol> <li>Output directory \u2014 All inference outputs (predictions, diagnostics, logs) are saved here. Create this directory before running.</li> <li>Number of forward steps \u2014 Total timesteps to run. Each step is 6 hours, so 1458 steps \u2248 365 days. See note below on limitation.</li> <li>Steps in memory \u2014 Batch size for GPU memory. Lower this if you run into OOM errors. 80 is a good default.</li> <li>Model checkpoint \u2014 Path to the pretrained ACE2-EAMv3 weights (<code>.tar</code> file from Hugging Face).</li> <li>Initial conditions \u2014 Starting atmospheric state. <code>n_initial_conditions</code> runs multiple ensemble members; <code>first</code> and <code>interval</code> control which samples to use from the IC file.</li> <li>Forcing data \u2014 External forcing (SST, solar, GHGs, etc.). The loader reads Zarr/NetCDF files from this path. <code>num_data_workers</code> controls parallel I/O.</li> <li>Logging options \u2014 <code>log_to_screen</code> prints progress; <code>log_to_wandb</code> sends metrics to Weights &amp; Biases (requires login); <code>log_to_file</code> saves to <code>inference_out.log</code>.</li> <li>Output writer \u2014 Set <code>save_prediction_files: true</code> to write NetCDF outputs. Set to <code>false</code> for validation-only runs. Additionally, one could <code>names: [T_4, T_5]</code> to request only <code>T_4</code> and <code>T_5</code> in the output.</li> </ol> <p>maximum steps</p> <p>Note that this is limited by the temporal length of the forcing data (in the example above, a year; see forcing_data) and the specifics of the initial conditions (in the example above, 2 seperated by a single time step starting from 0). That's why we have an offset of 2 steps from a full year in the prediction. If we have one initial conditions, then the number of forward steps would be 1459. The general formula is: <code>max steps allowed = length of data - (first + interval * (n_initial_conditions-1))</code></p> <p>Run the inference using the following command:</p> <pre><code>uv run python -m fme.ace.inference config-inference.yaml\n</code></pre> <p>compute node</p> <p>The above command takes about 10 minutes on a single compute node on pm-gpu (4xA100). The command to get a compute pm-gpu compute node is:  <pre><code>salloc --nodes 1 --qos interactive --time 04:00:00 --constraint gpu --account=e3sm_g\n</code></pre></p> <p>uv cache</p> <p>Sometimes, you will need to the enviornment variable <code>UV_CACHE_DIR</code>, e.g., on NERSC, <code>export UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"</code></p>"},{"location":"ace2-inference/#4-results","title":"4. Results","text":"<p>The results will be saved in the <code>experiment_dir</code> specified in the config file. The output directory structure will look like this:</p> <pre><code>&gt; ls /pscratch/sd/m/mahf708/ACE2-EAMv3/test1 -1 \nannual_diagnostics.nc\nautoregressive_predictions.nc\nautoregressive_target.nc\nconfig.yaml\ninference_out.log\ninitial_condition.nc\nmean_diagnostics.nc\nmonthly_mean_predictions.nc\nmonthly_mean_target.nc\nrestart.nc\ntime_mean_diagnostics.nc  \n</code></pre> <p>where the autoregressive_predictions.nc file has the following header:</p> <pre><code>ncdump -h /pscratch/sd/m/mahf708/ACE2-EAMv3/test1/autoregressive_predictions.nc\nnetcdf autoregressive_predictions {\ndimensions:\n        time = UNLIMITED ; // (1458 currently)\n        sample = 2 ;\n        lat = 180 ;\n        lon = 360 ;\nvariables:\n        int64 time(time) ;\n                time:units = \"microseconds\" ;\n        int64 init_time(sample) ;\n                init_time:units = \"microseconds since 1970-01-01 00:00:00\" ;\n                init_time:calendar = \"noleap\" ;\n        ...\n</code></pre> <p>These are the available variables:</p> <pre><code>&gt; ncdump -h /pscratch/sd/m/mahf708/ACE2-EAMv3/test1/autoregressive_predictions.nc | grep \"float\\|int64\" | awk '{print $2}' | cut -d'(' -f1\ntime\ninit_time\nvalid_time\nlat\nlon\nTS\nnet_energy_flux_sfc_into_atmosphere\nT_4\nV_2\nsurface_pressure_due_to_dry_air_absolute_tendency\nT_3\nT_0\nT_6\nV_4\nU_3\nU_4\ntendency_of_total_water_path_due_to_advection\nspecific_total_water_3\nspecific_total_water_4\nU_1\nV_7\nU_5\nFLUT\nsurface_precipitation_rate\nsurface_upward_longwave_flux\ntop_of_atmos_upward_shortwave_flux\nspecific_total_water_0\nT_7\nU_2\nnet_energy_flux_into_atmospheric_column\nspecific_total_water_1\nspecific_total_water_6\nV_0\ntotal_water_path\nV_1\nV_6\nFLDS\nFSDS\nPS\ntotal_energy_ace2_path\nT_1\nV_3\nsurface_pressure_due_to_dry_air\nU_6\nspecific_total_water_5\nV_5\nT_5\nU_0\nSHFLX\nLHFLX\nspecific_total_water_7\nsurface_upward_shortwave_flux\nT_2\nU_7\nspecific_total_water_2\ntotal_energy_ace2_path_tendency\ntotal_water_path_budget_residual\nimplied_tendency_of_total_energy_ace2_path_due_to_advection\nnet_energy_flux_toa_into_atmosphere\n</code></pre>"},{"location":"ace2-inference/#remaining-tasks","title":"Remaining tasks","text":"<ul> <li> Prepare forcing data for longer time period (e.g., 10 years)</li> <li> Explain the variables and files produces</li> <li> a restart.nc file is recorded. How do we perform a restart run?</li> <li> more information about the variables and levels</li> <li> Explore performance space, and producing larger ensembles</li> </ul>"},{"location":"ace2-workflow/","title":"ACE2-ERA5 Training Workflow","text":"<p>This guide provides a complete workflow for training the ACE (AI2 Climate Emulator) model using the ACE2-ERA5 dataset.</p>"},{"location":"ace2-workflow/#overview","title":"Overview","text":"<p>ACE is a machine learning model for climate emulation developed by AI2. This workflow will guide you through setting up the environment and running training on a GPU compute node.</p>"},{"location":"ace2-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to a compute cluster with GPU nodes (e.g., pm-gpu)</li> <li>Storage space for the dataset (ACE2-ERA5)</li> <li>Network access to clone repositories</li> </ul>"},{"location":"ace2-workflow/#resources","title":"Resources","text":"<ul> <li>Code Repository: E3SM-Project/ace</li> <li>Dataset: ACE2-ERA5 on Hugging Face</li> <li>Documentation: ACE Training Configuration Guide</li> </ul>"},{"location":"ace2-workflow/#setup-instructions","title":"Setup Instructions","text":""},{"location":"ace2-workflow/#1-clone-the-code-repository","title":"1. Clone the Code Repository","text":"<p>Start by cloning the ACE repository and checking out the main branch:</p> <pre><code>git clone https://github.com/E3SM-Project/ace.git\ncd ace\n</code></pre>"},{"location":"ace2-workflow/#2-download-the-dataset","title":"2. Download the Dataset","text":"<p>Clone the ACE2-ERA5 dataset from Hugging Face:</p> <pre><code>git clone https://huggingface.co/allenai/ACE2-ERA5\n</code></pre> <p>Dataset Size</p> <p>The ACE2-ERA5 dataset is large. Ensure you have sufficient storage space before cloning. Also, if you run into issues related to git lfs, you may need to install that.</p>"},{"location":"ace2-workflow/#3-install-uv-package-manager","title":"3. Install uv Package Manager","text":"<p>Install the <code>uv</code> package manager, which is used to manage Python dependencies:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>After installation, you may need to restart your console or source your profile to use <code>uv</code>.</p>"},{"location":"ace2-workflow/#4-configure-uv-cache","title":"4. Configure uv Cache","text":"<p>Set up the uv cache directory in an accessible location with sufficient storage:</p> <pre><code>mkdir -p \"$PSCRATCH/.cache/uv\"\nexport UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"\n</code></pre> <p>Cache Location</p> <p>Adjust <code>$PSCRATCH</code> to match your system's scratch directory path. This ensures the cache is stored in a location with adequate space.</p>"},{"location":"ace2-workflow/#5-pin-python-version","title":"5. Pin Python Version","text":"<p>Pin the Python version to 3.11:</p> <pre><code>uv python pin 3.11\n</code></pre>"},{"location":"ace2-workflow/#6-pin-setuptools-version","title":"6. Pin Setuptools Version","text":"<p>To avoid deprecation warnings related to <code>pkg_resources</code>, pin setuptools to a version below 81:</p> <pre><code>uv add --dev 'setuptools&lt;81'\n</code></pre> <p>pkg_resources Deprecation</p> <p>The ACE codebase and some of its dependencies use <code>pkg_resources</code>, which is deprecated and scheduled for removal in setuptools version 81+. Until the dependencies are updated to use <code>importlib.resources</code> or <code>importlib.metadata</code>, you must pin setuptools to version &lt;81 to avoid deprecation warnings and ensure compatibility.</p>"},{"location":"ace2-workflow/#running-training","title":"Running Training","text":""},{"location":"ace2-workflow/#1-request-a-gpu-compute-node","title":"1. Request a GPU Compute Node","text":"<p>Request an interactive GPU node on your cluster:</p> <pre><code>salloc --nodes 1 --qos interactive --time 04:00:00 --constraint gpu --account=e3sm_g\n</code></pre> <p>Account Settings</p> <p>Adjust the <code>--account</code> parameter to match your allocation account.</p>"},{"location":"ace2-workflow/#2-prepare-training-configuration","title":"2. Prepare Training Configuration","text":"<p>Create a training configuration file named <code>config-train.yaml</code> in the repository root. You can start with a template from the ACE training configuration documentation, or use the sample below:</p> Sample Configuration (<code>config-train.yaml</code>) <pre><code>experiment_dir: /path/to/your/ACE2-ERA5/train_output\nsave_checkpoint: true\nvalidate_using_ema: true\nmax_epochs: 80\nn_forward_steps: 1\ninference:\n  n_forward_steps: 300  # ~75 days (adjust based on your needs)\n  forward_steps_in_memory: 1\n  loader:\n    start_indices:\n      first: 0\n      n_initial_conditions: 4\n      interval: 300  # adjusted to fit within dataset\n    dataset:\n      data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\n    num_data_workers: 4\nlogging:\n  log_to_screen: true\n  log_to_wandb: false\n  log_to_file: true\n  project: ace\n  entity: your_wandb_entity\ntrain_loader:\n  batch_size: 4\n  num_data_workers: 2\n  prefetch_factor: 2\n  dataset:\n    concat:\n      - data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\nvalidation_loader:\n  batch_size: 4\n  num_data_workers: 2\n  prefetch_factor: 2\n  dataset:\n    data_path: /path/to/your/ACE2-ERA5/training_validation_data/training_validation\n    subset:\n      step: 5\noptimization:\n  enable_automatic_mixed_precision: false\n  lr: 0.0001\n  optimizer_type: AdamW\n  # can also set kwargs: fused: true for performance if using GPU\nstepper:\n  loss:\n    type: MSE\n  step:\n    type: single_module\n    config:\n      builder:\n        type: SphericalFourierNeuralOperatorNet\n        config:\n          embed_dim: 16\n          filter_type: linear\n          hard_thresholding_fraction: 1.0\n          use_mlp: true\n          normalization_layer: instance_norm\n          num_layers: 2\n          operator_type: dhconv\n          scale_factor: 1\n          separable: false\n      normalization:\n        network:\n          global_means_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/centering.nc\n          global_stds_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/scaling-full-field.nc\n        loss:\n          global_means_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/centering.nc\n          global_stds_path: /path/to/your/ACE2-ERA5/training_validation_data/normalization/scaling-residual.nc\n      in_names:\n      - land_fraction\n      - ocean_fraction\n      - sea_ice_fraction\n      - DSWRFtoa\n      - HGTsfc\n      - PRESsfc\n      - surface_temperature\n      - air_temperature_0 # _0 denotes the top most layer of the atmosphere\n      - air_temperature_1\n      - air_temperature_2\n      - air_temperature_3\n      - air_temperature_4\n      - air_temperature_5\n      - air_temperature_6\n      - air_temperature_7\n      - specific_total_water_0\n      - specific_total_water_1\n      - specific_total_water_2\n      - specific_total_water_3\n      - specific_total_water_4\n      - specific_total_water_5\n      - specific_total_water_6\n      - specific_total_water_7\n      - eastward_wind_0\n      - eastward_wind_1\n      - eastward_wind_2\n      - eastward_wind_3\n      - eastward_wind_4\n      - eastward_wind_5\n      - eastward_wind_6\n      - eastward_wind_7\n      - northward_wind_0\n      - northward_wind_1\n      - northward_wind_2\n      - northward_wind_3\n      - northward_wind_4\n      - northward_wind_5\n      - northward_wind_6\n      - northward_wind_7\n      out_names:\n      - PRESsfc\n      - surface_temperature\n      - air_temperature_0\n      - air_temperature_1\n      - air_temperature_2\n      - air_temperature_3\n      - air_temperature_4\n      - air_temperature_5\n      - air_temperature_6\n      - air_temperature_7\n      - specific_total_water_0\n      - specific_total_water_1\n      - specific_total_water_2\n      - specific_total_water_3\n      - specific_total_water_4\n      - specific_total_water_5\n      - specific_total_water_6\n      - specific_total_water_7\n      - eastward_wind_0\n      - eastward_wind_1\n      - eastward_wind_2\n      - eastward_wind_3\n      - eastward_wind_4\n      - eastward_wind_5\n      - eastward_wind_6\n      - eastward_wind_7\n      - northward_wind_0\n      - northward_wind_1\n      - northward_wind_2\n      - northward_wind_3\n      - northward_wind_4\n      - northward_wind_5\n      - northward_wind_6\n      - northward_wind_7\n      - LHTFLsfc\n      - SHTFLsfc\n      - PRATEsfc\n      - ULWRFsfc\n      - ULWRFtoa\n      - DLWRFsfc\n      - DSWRFsfc\n      - USWRFsfc\n      - USWRFtoa\n      - tendency_of_total_water_path_due_to_advection\n</code></pre> <p>Important: Make sure to update the following in your <code>config-train.yaml</code>:</p> <ul> <li><code>experiment_dir</code>: Set this to a writable directory where training outputs will be saved</li> <li><code>data_path</code>: Point this to your downloaded ACE2-ERA5 dataset location</li> </ul> <p>Fast Iteration</p> <p>For faster iteration during initial testing, consider:</p> <ul> <li>Reducing the number of training epochs</li> <li>Using a smaller batch size</li> <li>Limiting the dataset size</li> </ul>"},{"location":"ace2-workflow/#3-launch-training","title":"3. Launch Training","text":"<p>From the repository root, launch the training job using <code>torchrun</code>:</p> <pre><code>uv run torchrun --nproc_per_node=4 -m fme.ace.train config-train.yaml\n</code></pre> <p>This command will:</p> <ul> <li>Use <code>uv run</code> to manage dependencies automatically</li> <li>Launch <code>torchrun</code> with 4 processes (one per GPU)</li> <li>Execute the training module with your configuration</li> </ul>"},{"location":"ace2-workflow/#training-parameters","title":"Training Parameters","text":"<p>The <code>torchrun</code> command accepts several parameters:</p> <ul> <li><code>--nproc_per_node=4</code>: Number of processes per node (typically matches the number of GPUs)</li> <li><code>-m fme.ace.train</code>: The Python module to run</li> <li><code>config-train.yaml</code>: Your training configuration file</li> </ul>"},{"location":"ace2-workflow/#monitoring-training","title":"Monitoring Training","text":"<p>During training, monitor:</p> <ul> <li>GPU utilization: Use <code>nvidia-smi</code> to check GPU usage</li> <li>Training logs: Check the output directory specified in <code>experiment_dir</code></li> <li>Checkpoints: Models will be saved periodically based on your configuration</li> </ul>"},{"location":"ace2-workflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ace2-workflow/#common-issues","title":"Common Issues","text":"<p>pkg_resources Deprecation Warnings</p> <p>If you see warnings like: <pre><code>UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n</code></pre></p> <p>This occurs when setuptools version 81 or higher is installed. The ACE codebase and its dependencies still use <code>pkg_resources</code>, which is deprecated and scheduled for removal in setuptools 81+. To resolve this:</p> <ul> <li>Pin setuptools to version &lt;81: <code>uv add --dev 'setuptools&lt;81'</code></li> <li>Alternatively, suppress the warnings (not recommended) until dependencies are updated</li> </ul> <p>Out of Memory Errors</p> <ul> <li>Reduce batch size in <code>config-train.yaml</code></li> <li>Decrease model size parameters</li> <li>Use fewer GPUs with <code>--nproc_per_node</code></li> </ul> <p>Cache Directory Issues</p> <ul> <li>Ensure <code>UV_CACHE_DIR</code> has sufficient space</li> <li>Check write permissions on the cache directory</li> </ul> <p>Module Import Errors</p> <ul> <li>Verify Python version is pinned to 3.11</li> <li>Ensure you're running from the repository root</li> <li>Check that dependencies are properly installed by <code>uv</code></li> </ul>"},{"location":"ace2-workflow/#additional-resources","title":"Additional Resources","text":"<ul> <li>ACE Documentation</li> <li>uv Documentation</li> <li>PyTorch Distributed Training</li> </ul>"},{"location":"python-envs/","title":"Python Environment Setup","text":"<p>Quick guide for setting up Python environments to run AI group workflows. There are several options for setting up Python environments, including</p> <ul> <li>uv</li> <li>micromamba (and other conda variants)</li> <li>pip</li> </ul> <p>In this guide, we will only highlight uv, but users can achieve the same results with other tools.</p> <p>only a quick start</p> <p>This is simply a starting guide, and isn't meant to be exhaustive or performant.</p>"},{"location":"python-envs/#recommended-option-uv","title":"Recommended option: uv","text":"<p>The uv tool is a fast, modern Python package manager. It's significantly faster than pip and handles virtual environments elegantly.</p>"},{"location":"python-envs/#install-uv","title":"Install uv","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"python-envs/#clone-ace-and-install-env-manually","title":"Clone ACE and install env manually","text":"<pre><code># Clone ACE\ngit clone https://github.com/E3SM-Project/ace.git\ncd ace\n\n# Create venv with specific Python version\nuv venv --python 3.11\n\n# Activate\nsource .venv/bin/activate\n\n# Install dependencies from requirements.txt\nuv pip install -r requirements.txt\n\n# Install ACE in editable mode\nuv pip install -e .\n</code></pre>"},{"location":"python-envs/#clone-ace-and-run-automatically","title":"Clone ACE and run automatically","text":"<p>Instead of manual installation, you can clone ACE and then let uv handle the rest:</p> <pre><code># Clone ACE\ngit clone https://github.com/E3SM-Project/ace.git\ncd ace\n\nuv run python -m fme.ace.inference scratch/config-inference.yaml\n</code></pre> <p>The last command starting with <code>uv run</code> (documentation) ensures that the specified environment in the ACE repo is set and the trailing instruction (<code>python -m fme.ace.inference scratch/config-inference.yaml</code>) happens inside of it. This environment is still saved in <code>.venv</code> in the root of the repo.</p> <p>uv cache</p> <p>Sometimes, you will need to the enviornment variable <code>UV_CACHE_DIR</code>, e.g., on NERSC, <code>export UV_CACHE_DIR=\"$PSCRATCH/.cache/uv\"</code></p>"},{"location":"python-envs/#last-words","title":"Last words","text":"<p>In general, there's no one-size-fits-all solution for setting up Python environments. It really depends on your specific needs and preferences. Because Python is ubiquitous, the user must decide which tool to use for their specific needs, and learn to use it effectively for their needs.</p>"},{"location":"screamv1-example/","title":"SCREAM v1 Data Generation Example","text":"<p>In developing SCREAM v1, the team made a concerted effort to generalize and streamline variable output capability to allow for flexible and hopefully scientifically valuable capabilities. In this example, we will use SCREAM v1 data to generate all the data we need to train ACE2 on the fly (during runtime) without the need for post-processing or outputing expensive 3D variables. Please refer to ACE2-ERA5 Training Workflow for more information about ACE2 training.</p>"},{"location":"screamv1-example/#key-insights","title":"Key insights","text":"<p>We use the following capabilities from SCREAM v1:</p> <ul> <li>binary operations (<code>plus</code>, <code>over</code>)</li> <li>vertical reduction (<code>vert_avg</code>)</li> <li>conditional sampling (<code>where</code>)</li> <li>process tendencies (<code>homme_[x]_tend</code>)</li> <li>io aliasing (<code>:=</code>)</li> </ul> <p>All these capabilities are documented in EAMxx user guide.</p> <p>The <code>over</code> by constants is not yet in the mainline code. As of this writing, it is only available in the mahf708:hbc2 branch.</p>"},{"location":"screamv1-example/#specific-examples","title":"Specific examples","text":"<ol> <li><code>air_temperature_2:=T_mid_where_lev_ge_28_where_lev_lt_46_vert_avg_dp_weighted</code> defines a new variable <code>air_temperature_2</code>. It takes the <code>T_mid</code> (temperature at mid-levels) variable, applies a conditional filter to include only levels greater than or equal to 28 and less than 46, and then performs a vertical average weighted by pressure thickness (<code>dp</code>)</li> <li><code>total_specific_humidity_0:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_0_where_lev_lt_15_vert_avg_dp_weighted</code> defines a new variable <code>total_specific_humidity_0</code>. It takes the <code>qc</code> (cloud water), <code>qv</code> (water vapor), <code>qi</code> (ice), and <code>qr</code> (rain) variables, applies a conditional filter to include only levels greater than or equal to 0 and less than 15, and then performs a vertical average weighted by pressure thickness (<code>dp</code>).</li> <li><code>tendency_of_total_water_path_due_to_advection:=homme_qc_tend_plus_homme_qv_tend_plus_homme_qi_tend_plus_homme_qr_tend_vert_avg_dp_weighted</code> defines a new variable <code>tendency_of_total_water_path_due_to_advection</code>. It takes the <code>homme_qc_tend</code> (cloud water tendency), <code>homme_qv_tend</code> (water vapor tendency), <code>homme_qi_tend</code> (ice tendency), and <code>homme_qr_tend</code> (rain tendency) variables, and then performs a vertical average weighted by pressure thickness (<code>dp</code>).</li> <li><code>HGTsfc:=phis_over_gravit</code> defines a new variable <code>HGTsfc</code>. It takes the <code>phis</code> (surface geopotential) variable, and dvides by the gravitation acceleration (taken as constant <code>gravit</code>) to get surface height.</li> </ol>"},{"location":"screamv1-example/#full-script","title":"Full script","text":"<p>A full script can be found below.</p> SCREAM v1 run script <pre><code>#!/bin/bash -fe\n\n# EAMxx template run script\n\nmain() {\n\ndo_fetch_code=false\ndo_create_newcase=true\ndo_case_setup=true\ndo_case_build=true\ndo_case_submit=true\n\nreadonly MACHINE=\"pm-gpu\"\nreadonly CHECKOUT=\"10022025\"\nreadonly BRANCH=\"hbc2\"\nreadonly CHERRY=( )\nreadonly COMPILER=\"gnugpu\"\nreadonly DEBUG_COMPILE=FALSE\nreadonly Q=debug\nreadonly QUEUE=${Q}\n\n# Simulation\nreadonly COMPSET=\"F2010-SCREAMv1\"\nreadonly RESOLUTION=\"ne30pg2_ne30pg2\"\n\nreadonly CODE_ROOT=\"/pscratch/sd/m/mahf708/e3sm-repo/test-pr\"\nreadonly PROJECT=\"e3sm\"\n\nreadonly TUNINGSET=\"default\" # also plus4k and minus4k are options\n\nreadonly CASE_NAME=${RESOLUTION}.${COMPSET}.${CHECKOUT}.${TUNINGSET}\n\nreadonly CASE_ROOT=\"${SCRATCH}/e3sm_scratch/${MACHINE}/${CASE_NAME}\"\n\ngithash_eamxx=`git --git-dir ${CODE_ROOT}/.git rev-parse HEAD`\n\n\n\n# ****************** Lines to be modified ********************************\n\n\n# ****************** Lines to be modified ********************************\n\n\n# History file frequency (if using default above)\nreadonly HIST_OPTION=\"nmonths\"\nreadonly HIST_N=\"1\"\n\n# Run options\nreadonly MODEL_START_TYPE=\"initial\"  # \"initial\", \"continue\", \"branch\", \"hybrid\"\nreadonly START_DATE=\"2000-01-01\"     # \"\" for default, or explicit \"0001-01-01\"\n\n# Additional options for 'branch' and 'hybrid'\nreadonly GET_REFCASE=false\nreadonly RUN_REFDIR=\"\"\nreadonly RUN_REFCASE=\"\"\nreadonly RUN_REFDATE=\"\"   # same as MODEL_START_DATE for 'branch', can be different for 'hybrid'\n\n\n# Sub-directories\nreadonly CASE_BUILD_DIR=${CASE_ROOT}/build\nreadonly CASE_ARCHIVE_DIR=${CASE_ROOT}/archive\n\nreadonly CASE_SCRIPTS_DIR=${CASE_ROOT}/case_scripts\nreadonly CASE_RUN_DIR=${CASE_ROOT}/run\n\nreadonly NUM_PES=4\nif [[ ${MPS_YES} == \"TRUE\" ]]; then\n    readonly PEL_SIM=$(( MPS_NOS * NUM_PES ))\nelse\n    readonly PEL_SIM=$(( 4 * NUM_PES ))\nfi\nreadonly PELAYOUT=${PEL_SIM}\"x1\"\n\nreadonly WALLTIME=\"00:25:00\"\nreadonly STOP_OPTION=\"nyears\"\nreadonly STOP_N=\"10\"\nreadonly REST_OPTION=\"nyears\"\nreadonly REST_N=\"1\"\nreadonly RESUBMIT=\"0\"\nreadonly DO_SHORT_TERM_ARCHIVING=false\n\n# Leave empty (unless you understand what it does)\nreadonly OLD_EXECUTABLE=\"\"\n\n# --- Now, do the work ---\n\n# Make directories created by this script world-readable\numask 022\n\n# Fetch code from Github\nfetch_code\n\n# Create case\ncreate_newcase\n\n# Setup\ncase_setup\n\n# Build\ncase_build\n\n# Configure runtime options\nruntime_options\n\n# Copy script into case_script directory for provenance\ncopy_script\n\n# Submit\ncase_submit\n\n# All done\necho $'\\n----- All done -----\\n'\n\n}\n\n# =======================\n# Custom user_nl settings\n# =======================\n\nuser_nl() {\n\n    echo \"+++ Configuring SCREAM for 128 vertical levels +++\"\n    ./xmlchange SCREAM_CMAKE_OPTIONS=\"SCREAM_NP 4 SCREAM_NUM_VERTICAL_LEV 128 SCREAM_NUM_TRACERS 10\"\n\n}\n\n######################################################\n### Most users won't need to change anything below ###\n######################################################\n\n#-----------------------------------------------------\nfetch_code() {\n\n    if [ \"${do_fetch_code,,}\" != \"true\" ]; then\n    echo $'\\n----- Skipping fetch_code -----\\n'\n    return\n    fi\n\n    echo $'\\n----- Starting fetch_code -----\\n'\n    local path=${CODE_ROOT}\n    local repo=scream\n\n    echo \"Cloning $repo repository branch $BRANCH under $path\"\n    if [ -d \"${path}\" ]; then\n    echo \"ERROR: Directory already exists. Not overwriting\"\n    exit 20\n    fi\n    mkdir -p ${path}\n    pushd ${path}\n\n    # This will put repository, with all code\n    git clone git@github.com:E3SM-Project/${repo}.git .\n\n    # Q: DO WE NEED THIS FOR EAMXX?\n    # Setup git hooks\n    rm -rf .git/hooks\n    git clone git@github.com:E3SM-Project/E3SM-Hooks.git .git/hooks\n    git config commit.template .git/hooks/commit.template\n\n    # Check out desired branch\n    git checkout ${BRANCH}\n\n    # Custom addition\n    if [ \"${CHERRY}\" != \"\" ]; then\n    echo ----- WARNING: adding git cherry-pick -----\n    for commit in \"${CHERRY[@]}\"\n    do\n        echo ${commit}\n        git cherry-pick ${commit}\n    done\n    echo -------------------------------------------\n    fi\n\n    # Bring in all submodule components\n    git submodule update --init --recursive\n\n    popd\n}\n\n#-----------------------------------------------------\ncreate_newcase() {\n\n    if [ \"${do_create_newcase,,}\" != \"true\" ]; then\n    echo $'\\n----- Skipping create_newcase -----\\n'\n    return\n    fi\n\n    echo $'\\n----- Starting create_newcase -----\\n'\n\n    # Base arguments\n    args=\" --case ${CASE_NAME} \\\n    --output-root ${CASE_ROOT} \\\n    --script-root ${CASE_SCRIPTS_DIR} \\\n    --handle-preexisting-dirs u \\\n    --compset ${COMPSET} \\\n    --res ${RESOLUTION} \\\n    --machine ${MACHINE} \\\n    --compiler ${COMPILER} \\\n    --walltime ${WALLTIME} \\\n    --pecount ${PELAYOUT}\"\n\n    # Oprional arguments\n    if [ ! -z \"${PROJECT}\" ]; then\n    args=\"${args} --project ${PROJECT}\"\n    fi\n    if [ ! -z \"${CASE_GROUP}\" ]; then\n    args=\"${args} --case-group ${CASE_GROUP}\"\n    fi\n    if [ ! -z \"${QUEUE}\" ]; then\n    args=\"${args} --queue ${QUEUE}\"\n    fi\n\n    ${CODE_ROOT}/cime/scripts/create_newcase ${args}\n\n    if [ $? != 0 ]; then\n    echo $'\\nNote: if create_newcase failed because sub-directory already exists:'\n    echo $'  * delete old case_script sub-directory'\n    echo $'  * or set do_newcase=false\\n'\n    exit 35\n    fi\n\n}\n\n#-----------------------------------------------------\ncase_setup() {\n\n    if [ \"${do_case_setup,,}\" != \"true\" ]; then\n    echo $'\\n----- Skipping case_setup -----\\n'\n    return\n    fi\n\n    echo $'\\n----- Starting case_setup -----\\n'\n    pushd ${CASE_SCRIPTS_DIR}\n\n    # Setup some CIME directories\n    ./xmlchange EXEROOT=${CASE_BUILD_DIR}\n    ./xmlchange RUNDIR=${CASE_RUN_DIR}\n\n    # Short term archiving\n    ./xmlchange DOUT_S=${DO_SHORT_TERM_ARCHIVING}\n    ./xmlchange DOUT_S_ROOT=${CASE_ARCHIVE_DIR}\n\n    # Extracts input_data_dir in case it is needed for user edits to the namelist later\n    local input_data_dir=`./xmlquery DIN_LOC_ROOT --value`\n\n    # Custom user_nl\n    user_nl\n\n    MPS_NUMBER=4\n\n    ./xmlchange --file env_mach_pes.xml NTHRDS=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_ATM=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_LND=\"$(( 64 / MPS_NUMBER ))\"\n    ./xmlquery NTHRDS_LND\n    ./xmlchange --file env_mach_pes.xml NTHRDS_ICE=\"$(( 64 / MPS_NUMBER ))\"\n    ./xmlquery NTHRDS_ICE\n    ./xmlchange --file env_mach_pes.xml NTHRDS_OCN=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_ROF=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_CPL=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_GLC=\"1\"\n    ./xmlchange --file env_mach_pes.xml NTHRDS_WAV=\"1\"\n\n\n    ./xmlchange PIO_NETCDF_FORMAT=\"64bit_data\"\n\n    # Finally, run CIME case.setup\n    ./case.setup --reset\n\n    # Save provenance invfo\n    echo \"branch hash for EAMxx: $githash_eamxx\" &gt; GIT_INFO.txt\n    echo \"master hash for output files: $githash_screamdocs\" &gt;&gt; GIT_INFO.txt\n\n    popd\n}\n\n#-----------------------------------------------------\ncase_build() {\n\n    pushd ${CASE_SCRIPTS_DIR}\n\n    # do_case_build = false\n    if [ \"${do_case_build,,}\" != \"true\" ]; then\n\n    echo $'\\n----- case_build -----\\n'\n\n    if [ \"${OLD_EXECUTABLE}\" == \"\" ]; then\n        # Ues previously built executable, make sure it exists\n        if [ -x ${CASE_BUILD_DIR}/e3sm.exe ]; then\n        echo 'Skipping build because $do_case_build = '${do_case_build}\n        else\n        echo 'ERROR: $do_case_build = '${do_case_build}' but no executable exists for this case.'\n        exit 297\n        fi\n    else\n        # If absolute pathname exists and is executable, reuse pre-exiting executable\n        if [ -x ${OLD_EXECUTABLE} ]; then\n        echo 'Using $OLD_EXECUTABLE = '${OLD_EXECUTABLE}\n        cp -fp ${OLD_EXECUTABLE} ${CASE_BUILD_DIR}/\n        else\n        echo 'ERROR: $OLD_EXECUTABLE = '$OLD_EXECUTABLE' does not exist or is not an executable file.'\n        exit 297\n        fi\n    fi\n    echo 'WARNING: Setting BUILD_COMPLETE = TRUE.  This is a little risky, but trusting the user.'\n    ./xmlchange BUILD_COMPLETE=TRUE\n\n    # do_case_build = true\n    else\n\n    echo $'\\n----- Starting case_build -----\\n'\n\n    # Turn on debug compilation option if requested\n    if [ \"${DEBUG_COMPILE}\" == \"TRUE\" ]; then\n        ./xmlchange DEBUG=${DEBUG_COMPILE}\n    fi\n\n    # Run CIME case.build\n    ./case.build\n\n    # Some user_nl settings won't be updated to *_in files under the run directory\n    # Call preview_namelists to make sure *_in and user_nl files are consistent.\n    ./preview_namelists\n\n    fi\n\n    popd\n}\n\n#-----------------------------------------------------\nruntime_options() {\n\n    echo $'\\n----- Starting runtime_options -----\\n'\n    pushd ${CASE_SCRIPTS_DIR}\n\n    local input_data_dir=`./xmlquery DIN_LOC_ROOT --value`\n\n    # Set simulation start date\n    if [ ! -z \"${START_DATE}\" ]; then\n    ./xmlchange RUN_STARTDATE=${START_DATE}\n    fi\n\n\n\n    ./atmchange homme::compute_tendencies=qr,qv,qi,qc\n    ./atmchange eamxx::compute_tendencies=qr,qv,qi,qc\n\n\ncat &lt;&lt; EOF &gt;&gt; user_nl_elm\n\nhist_dov2xy = .true.,.true.\nhist_fincl2 = 'H2OSNO','SOILWATER_10CM','TG'\nhist_mfilt = 1,120\nhist_nhtfrq = 0,-24\nhist_avgflag_pertape = 'A','A'\n\nEOF\n\ncat &lt;&lt; EOF &gt;&gt; user_nl_cpl\nocn_surface_flux_scheme = 2\nEOF\n\n    # Segment length\n    ./xmlchange STOP_OPTION=${STOP_OPTION,,},STOP_N=${STOP_N}\n\n    # Restart frequency\n    ./xmlchange REST_OPTION=${REST_OPTION,,},REST_N=${REST_N}\n\n    # Coupler history\n    ./xmlchange HIST_OPTION=${HIST_OPTION,,},HIST_N=${HIST_N}\n\n    # Coupler budgets (always on)\n    ./xmlchange BUDGETS=TRUE\n\n    # Set resubmissions\n    if (( RESUBMIT &gt; 0 )); then\n    ./xmlchange RESUBMIT=${RESUBMIT}\n    fi\n\n    # Run type\n    # Start from default of user-specified initial conditions\n    if [ \"${MODEL_START_TYPE,,}\" == \"initial\" ]; then\n    ./xmlchange RUN_TYPE=\"startup\"\n    ./xmlchange CONTINUE_RUN=\"FALSE\"\n\n    # Continue existing run\n    elif [ \"${MODEL_START_TYPE,,}\" == \"continue\" ]; then\n    ./xmlchange CONTINUE_RUN=\"TRUE\"\n\n    elif [ \"${MODEL_START_TYPE,,}\" == \"branch\" ] || [ \"${MODEL_START_TYPE,,}\" == \"hybrid\" ]; then\n    ./xmlchange RUN_TYPE=${MODEL_START_TYPE,,}\n    ./xmlchange GET_REFCASE=${GET_REFCASE}\n    ./xmlchange RUN_REFDIR=${RUN_REFDIR}\n    ./xmlchange RUN_REFCASE=${RUN_REFCASE}\n    ./xmlchange RUN_REFDATE=${RUN_REFDATE}\n    echo 'Warning: $MODEL_START_TYPE = '${MODEL_START_TYPE}\n    echo '$RUN_REFDIR = '${RUN_REFDIR}\n    echo '$RUN_REFCASE = '${RUN_REFCASE}\n    echo '$RUN_REFDATE = '${START_DATE}\n\n    else\n    echo 'ERROR: $MODEL_START_TYPE = '${MODEL_START_TYPE}' is unrecognized. Exiting.'\n    exit 380\n    fi\n\ncat &lt;&lt; EOF &gt;&gt; 6hi.yaml\naveraging_type: instant\nfields:\nphysics_pg2:\n    field_names:\n    ####\n    - air_temperature_0:=T_mid_where_lev_ge_0_where_lev_lt_15_vert_avg_dp_weighted\n    - air_temperature_1:=T_mid_where_lev_ge_15_where_lev_lt_28_vert_avg_dp_weighted\n    - air_temperature_2:=T_mid_where_lev_ge_28_where_lev_lt_46_vert_avg_dp_weighted\n    - air_temperature_3:=T_mid_where_lev_ge_46_where_lev_lt_62_vert_avg_dp_weighted\n    - air_temperature_4:=T_mid_where_lev_ge_62_where_lev_lt_78_vert_avg_dp_weighted\n    - air_temperature_5:=T_mid_where_lev_ge_78_where_lev_lt_96_vert_avg_dp_weighted\n    - air_temperature_6:=T_mid_where_lev_ge_96_where_lev_lt_115_vert_avg_dp_weighted\n    - air_temperature_7:=T_mid_where_lev_ge_115_where_lev_le_128_vert_avg_dp_weighted\n    ####\n    - eastward_wind_0:=U_where_lev_ge_0_where_lev_lt_15_vert_avg_dp_weighted\n    - eastward_wind_1:=U_where_lev_ge_15_where_lev_lt_28_vert_avg_dp_weighted\n    - eastward_wind_2:=U_where_lev_ge_28_where_lev_lt_46_vert_avg_dp_weighted\n    - eastward_wind_3:=U_where_lev_ge_46_where_lev_lt_62_vert_avg_dp_weighted\n    - eastward_wind_4:=U_where_lev_ge_62_where_lev_lt_78_vert_avg_dp_weighted\n    - eastward_wind_5:=U_where_lev_ge_78_where_lev_lt_96_vert_avg_dp_weighted\n    - eastward_wind_6:=U_where_lev_ge_96_where_lev_lt_115_vert_avg_dp_weighted\n    - eastward_wind_7:=U_where_lev_ge_115_where_lev_le_128_vert_avg_dp_weighted\n    ####\n    - northward_wind_0:=V_where_lev_ge_0_where_lev_lt_15_vert_avg_dp_weighted\n    - northward_wind_1:=V_where_lev_ge_15_where_lev_lt_28_vert_avg_dp_weighted\n    - northward_wind_2:=V_where_lev_ge_28_where_lev_lt_46_vert_avg_dp_weighted\n    - northward_wind_3:=V_where_lev_ge_46_where_lev_lt_62_vert_avg_dp_weighted\n    - northward_wind_4:=V_where_lev_ge_62_where_lev_lt_78_vert_avg_dp_weighted\n    - northward_wind_5:=V_where_lev_ge_78_where_lev_lt_96_vert_avg_dp_weighted\n    - northward_wind_6:=V_where_lev_ge_96_where_lev_lt_115_vert_avg_dp_weighted\n    - northward_wind_7:=V_where_lev_ge_115_where_lev_le_128_vert_avg_dp_weighted\n    ####\n    - total_specific_humidity_0:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_0_where_lev_lt_15_vert_avg_dp_weighted\n    - total_specific_humidity_1:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_15_where_lev_lt_28_vert_avg_dp_weighted\n    - total_specific_humidity_2:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_28_where_lev_lt_46_vert_avg_dp_weighted\n    - total_specific_humidity_3:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_46_where_lev_lt_62_vert_avg_dp_weighted\n    - total_specific_humidity_4:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_62_where_lev_lt_78_vert_avg_dp_weighted\n    - total_specific_humidity_5:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_78_where_lev_lt_96_vert_avg_dp_weighted\n    - total_specific_humidity_6:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_96_where_lev_lt_115_vert_avg_dp_weighted\n    - total_specific_humidity_7:=qc_plus_qv_plus_qi_plus_qr_where_lev_ge_115_where_lev_le_128_vert_avg_dp_weighted\n    ####\n    - PRESsfc:=ps\n    - surface_temperature:=surf_radiative_T\n    ####\n    - land_fraction:=landfrac\n    - ocean_fraction:=ocnfrac\n    - sea_ice_fraction:=icefrac\n    ####\n    - tendency_of_total_water_path_due_to_advection:=homme_qc_tend_plus_homme_qv_tend_plus_homme_qi_tend_plus_homme_qr_tend_vert_avg_dp_weighted\n    ####\nmax_snapshots_per_file: 4\nfilename_prefix: 6hi\niotype: pnetcdf\noutput_control:\nfrequency: 6\nfrequency_units: nhours\nrestart:\nforce_new_file: true\nEOF\n\ncat &lt;&lt; EOF &gt;&gt; 6ha.yaml\naveraging_type: average\nfields:\nphysics_pg2:\n    field_names:\n    ####\n    - DSWRFtoa:=SW_flux_dn_at_model_top\n    - DSWRFsfc:=SW_flux_dn_at_model_bot\n    ####\n    - DLWRFtoa:=LW_flux_dn_at_model_top\n    - DLWRFsfc:=LW_flux_dn_at_model_bot\n    ####\n    - ULWRFtoa:=LW_flux_up_at_model_top\n    - ULWRFsfc:=LW_flux_up_at_model_bot\n    ####\n    - USWRFtoa:=SW_flux_up_at_model_top\n    - USWRFsfc:=SW_flux_up_at_model_bot\n    ####\n    - LHTFLsfc:=surface_upward_latent_heat_flux\n    - SHTFLsfc:=surf_sens_flux\n    ####\n    - PRATEsfc:=precip_total_surf_mass_flux_over_rho_h2o\n    ####\n    - HGTsfc:=phis_over_gravit\n    ####\nmax_snapshots_per_file: 4\nfilename_prefix: 6ha\niotype: pnetcdf\noutput_control:\nfrequency: 6\nfrequency_units: nhours\nrestart:\nforce_new_file: true\nEOF\n\n    ./atmchange output_yaml_files=\"./6hi.yaml\"\n    ./atmchange output_yaml_files+=\"./6ha.yaml\"\n\n    if [[ ${TUNINGSET} == \"default\" ]]; then\n    echo \"--------------------------------------------------------------------\"\n    echo \"---------------------USING default SST------------------------------\"\n    echo \"--------------------------------------------------------------------\"\n    ./xmlchange --file env_run.xml --id SSTICE_DATA_FILENAME --val \"${input_data_dir}/ocn/docn7/SSTDATA/sst_ice_CMIP6_DECK_E3SM_1x1_2010_clim_c20220426.nc\"\n    elif [[ ${TUNINGSET} == \"plus1k\" ]]; then\n    echo \"--------------------------------------------------------------------\"\n    echo \"---------------------USING P1K SST----------------------------------\"\n    echo \"--------------------------------------------------------------------\"\n    ./xmlchange --file env_run.xml --id SSTICE_DATA_FILENAME --val \"${input_data_dir}/ocn/docn7/SSTDATA/sst_ice_CMIP6_DECK_E3SM_1x1_2010_clim_plus1k_c20220426.nc\"\n    elif [[ ${TUNINGSET} == \"minus1k\" ]]; then\n    echo \"--------------------------------------------------------------------\"\n    echo \"---------------------USING M1K SST----------------------------------\"\n    echo \"--------------------------------------------------------------------\"\n    ./xmlchange --file env_run.xml --id SSTICE_DATA_FILENAME --val \"${input_data_dir}/ocn/docn7/SSTDATA/sst_ice_CMIP6_DECK_E3SM_1x1_2010_clim_minus1k_c20220426.nc\"\n    fi\n\n    popd\n}\n\n#-----------------------------------------------------\ncase_submit() {\n\n    if [ \"${do_case_submit,,}\" != \"true\" ]; then\n    echo $'\\n----- Skipping case_submit -----\\n'\n    return\n    fi\n\n    echo $'\\n----- Starting case_submit -----\\n'\n    pushd ${CASE_SCRIPTS_DIR}\n\n    # Run CIME case.submit\n    ./case.submit -a=\"--mail-type=ALL --mail-user=$USER@nersc.gov\"\n    #./case.submit -a=\"--qos=${Q}\"\n\n    popd\n}\n\n#-----------------------------------------------------\ncopy_script() {\n\n    echo $'\\n----- Saving run script for provenance -----\\n'\n\n    local script_provenance_dir=${CASE_SCRIPTS_DIR}/run_script_provenance\n    mkdir -p ${script_provenance_dir}\n    local this_script_name=$( basename -- \"$0\"; )\n    local this_script_dir=$( dirname -- \"$0\"; )\n    local script_provenance_name=${this_script_name}.`date +%Y%m%d-%H%M%S`\n    cp -vp \"${this_script_dir}/${this_script_name}\" ${script_provenance_dir}/${script_provenance_name}\n\n}\n\n#-----------------------------------------------------\n# Silent versions of popd and pushd\npushd() {\n    command pushd \"$@\" &gt; /dev/null\n}\npopd() {\n    command popd \"$@\" &gt; /dev/null\n}\n\n# Now, actually run the script\n#-----------------------------------------------------\nmain\n</code></pre>"}]}